{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Default settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/john/.local/lib/python3.8/site-packages/nilearn/datasets/__init__.py:86: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import decimal\n",
    "import pathlib\n",
    "import operator\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from nilearn.image import iter_img\n",
    "from nilearn.decomposition import CanICA\n",
    "from nilearn.masking import apply_mask\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "from scipy.interpolate import RegularGridInterpolator"
   ]
  },
  {
   "source": [
    "# config defaults\n",
    "default_config = { \n",
    "    \"SKULLSTRIP\" : \"1\",\n",
    "    \"SLICETIME\"  : \"1\",\n",
    "    \"MOTCOR\"     : \"1\",\n",
    "    \"NORM\"       : \"1\",\n",
    "    \"SMOOTH\"     : \"6\", # gaussian smoothing kernel size in mm\n",
    "    \"MOTREG\"     : \"1\",\n",
    "    \"GSR\"        : \"1\",\n",
    "    \"NUISANCE\"   : \"3\",\n",
    "    \"TEMPLATE\"   : \"/usr/local/fsl/data/standard/MNI152_T1_2mm_brain.nii.gz\",\n",
    "    \"SCRUB\"      : \"UNION\" # union, intersect, dvars, fd, none\n",
    "}"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_order={\n",
    "    \"BASELINE\"  :0,\n",
    "    \"SKULLSTRIP\":1,\n",
    "    \"SLICETIME\" :2,\n",
    "    \"MOTCOR\"    :3,\n",
    "    \"NORM\"      :4,\n",
    "    \"GSR\"       :5,\n",
    "    \"MOTREG\"    :6,\n",
    "    \"NUISANCE\"  :7,\n",
    "    \"SCRUB\"     :8,\n",
    "    \"SMOOTH\"    :9\n",
    "}"
   ]
  },
  {
   "source": [
    "# Support functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirstruct(output):\n",
    "    pathlib.Path(os.path.join(output, \"func\"           )).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(os.path.join(output, \"motion\"         )).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(os.path.join(output, \"spat_norm\"      )).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(os.path.join(output, \"quality_control\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(os.path.join(output, \"anat\", \"segment\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_exist(filepath, type=\"any\"):\n",
    "    if type in [\"file\", \"f\"]:\n",
    "        from os.path import isfile as exists\n",
    "    elif type==\"dir\" or type==\"d\":\n",
    "        from os.path import isdir as exists\n",
    "    elif type==\"any\":\n",
    "        from os.path import exists\n",
    "    else:\n",
    "        raise Exception(\"type '\" + type + \"' is invalid.\")\n",
    "    \n",
    "    return(exists(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(text, keywords, split=\"=\"):\n",
    "    key, val = text.split(\"=\")\n",
    "    if key not in keywords:\n",
    "        raise Exception(\"Input doesn't contain any of the specified key words\")\n",
    "    return(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_chars(text, first, last):\r\n",
    "    text = \"{}\".format(text[1:] if text.startswith(first) else text)\r\n",
    "    text = \"{}\".format(text[:-1] if text.endswith(last) else text)\r\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input(text, keywords, split=\"=\", first=\"[\", last=\"]\"):\n",
    "    key, val = parse_line(text, keywords, split)\n",
    "    val = strip_chars(val, \"[\", \"]\")\n",
    "    return(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_defaults(config_dict, defaults):\n",
    "    default_keys=[key for key in defaults.keys()]\n",
    "\n",
    "    restore_defaults=[key not in config_dict.keys() for key in default_keys]\n",
    "    restore_defaults=list(filter(lambda x: restore_defaults[x], range(len(restore_defaults))))\n",
    "    restore_defaults=[default_keys[i] for i in restore_defaults]\n",
    "\n",
    "    for key in restore_defaults:\n",
    "        config_dict[key] = defaults[key]\n",
    "\n",
    "    return(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxel_size(img, excl_time=True):\n",
    "    nii = nib.load(img)\n",
    "    vox_sz = nii.header.get_zooms()\n",
    "\n",
    "    if excl_time:\n",
    "        vox_sz=vox_sz[0:3]\n",
    "    return(vox_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_input_line(line, keywords):\n",
    "    input_dict = {}\n",
    "    items = line.split(\",\")\n",
    "    \n",
    "    for item in items:\n",
    "        item=item.strip()\n",
    "        try:\n",
    "            key, val = parse_input(item, keywords) # keywords must be enterest as a list\n",
    "            input_dict[key] = val\n",
    "        except:\n",
    "            print(\"Error:\", item)\n",
    "\n",
    "    # check if all files exist \n",
    "    if not check_exist(input_dict[\"FUNC\"], \"file\"):\n",
    "        raise Exception(input_dict[\"FUNC\"] + \" is not a valid filepath.\")\n",
    "    if not check_exist(input_dict[\"ANAT\"], \"file\"):\n",
    "        raise Exception(input_dict[\"ANAT\"] + \" is not a valid filepath.\")\n",
    "       \n",
    "    return(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_input(filepath):\n",
    "    input = open(filepath, \"r\")\n",
    "    lines = input.readlines()\n",
    "\n",
    "    keywords = [\"FUNC\", \"ANAT\", \"OUTPUT\"]\n",
    "    input_df = pd.DataFrame(index=range(len(lines)), columns=keywords)\n",
    "\n",
    "    for line, ind in zip(lines, range(len(lines))):\n",
    "        line=line.strip()\n",
    "        input_dict=interpret_input_line(line, keywords)\n",
    "\n",
    "        input_df.loc[ind,\"FUNC\"  ] = input_dict[\"FUNC\"]\n",
    "        input_df.loc[ind,\"ANAT\"  ] = input_dict[\"ANAT\"]\n",
    "        input_df.loc[ind,\"OUTPUT\"] = input_dict[\"OUTPUT\"]\n",
    "\n",
    "    return(input_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_config(filepath):\n",
    "    config=open(filepath, \"r\")\n",
    "    lines =config.readlines()\n",
    "    config_dict = {}\n",
    "\n",
    "    for line in lines:\n",
    "        line=line.strip()\n",
    "        try:\n",
    "            key, val = parse_input(line, default_config.keys) # keywords must be enterest as a list\n",
    "            config_dict[key] = val\n",
    "        except:\n",
    "            print(\"Error:\", line)\n",
    "\n",
    "    config_dict = check_defaults(config_dict, default_config)\n",
    "    return(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afni2nifti(filepath, rm_orig=True):    \n",
    "    nii_filepath=os.path.join(os.path.dirname(filepath), rm_ext(filepath))+\".nii\"\n",
    "    os.system(\"3dAFNItoNIFTI -prefix {} {}\".format(nii_filepath, filepath+\"*.HEAD\"))\n",
    "    if rm_orig:\n",
    "        os.system(\"rm {}*.HEAD\".format(filepath))\n",
    "        os.system(\"rm {}*.BRIK\".format(filepath))\n",
    "    return(nii_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTR(filepath):    \n",
    "    img = nib.load(filepath)\n",
    "    tr  = img.header.get_zooms()[3]\n",
    "    return(str(tr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_ext(filename):\n",
    "    filename=os.path.basename(filename)\n",
    "    newpath =os.path.splitext(filename)[0]\n",
    "    \n",
    "    if \".\" in newpath:\n",
    "        newpath=rm_ext(newpath)\n",
    "    return(newpath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ext(filename):\n",
    "    filename    =os.path.basename(filename)\n",
    "    newname, ext=os.path.splitext(filename)\n",
    "    \n",
    "    if \".\" in newname:\n",
    "        ext = get_ext(newname) + ext\n",
    "    return(ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_mm2sigma(mm):\n",
    "    return mm/2.35482004503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which(list, x=True):\n",
    "    iter=0\n",
    "    coords=[]\n",
    "    \n",
    "    for elem in list:\n",
    "        if elem == x:\n",
    "            coords.append(iter)\n",
    "        iter+=1\n",
    "    return(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gzip_file(filename, rm_orig=True):\n",
    "    with open(filename, 'rb') as f_in:\n",
    "        with gzip.open(filename+'.gz', 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        if rm_orig:\n",
    "            os.remove(filename)\n",
    "    return(filename+'.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val, my_dict):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(my_dict, ref_dict, which=\"prev\", ignore=[\"BASELINE\"], give=\"key\"):\n",
    "    if ignore is None:\n",
    "        ignore=[]\n",
    "    if which == \"first\":\n",
    "        target=min\n",
    "    elif which == \"prev\":\n",
    "        target=max\n",
    "    my_keys = list(my_dict.keys())\n",
    "    for ig in ignore:\n",
    "        if ig in my_keys:\n",
    "            pass\n",
    "        my_keys.remove(ig)\n",
    "    \n",
    "    key_val = [ref_dict[key] for key in my_keys]\n",
    "    if give==\"key\":\n",
    "        return get_key(target(key_val), ref_dict)\n",
    "    elif give==\"value\":\n",
    "        return target(key_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_rename(filepath, newpath):\n",
    "    os.system(\"cp {} {}\".format(filepath, newpath))\n",
    "    return newpath"
   ]
  },
  {
   "source": [
    "# Preprocessing Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brainroi(img, out_dir):\n",
    "    roi_img = os.path.join(out_dir, \"roi_\"+os.path.basename(img))\n",
    "    os.system(\"robustfov -i {} -r {}\".format(img, roi_img))\n",
    "\n",
    "    return(roi_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skullstrip(img, out_dir):\n",
    "    out_dir  = os.path.join(out_dir, \"anat\")\n",
    "    out_file = \"brain_\" + rm_ext(img)\n",
    "    skullstr = os.path.join(out_dir, out_file)\n",
    "\n",
    "    command=\"bet {} {} -R\".format(img, skullstr)\n",
    "    os.system(command)\n",
    "    print(\"       \"+command)\n",
    "\n",
    "    return(skullstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicetime(img, out_dir):\n",
    "    tr=getTR(img)\n",
    "    tshift_path=os.path.join(out_dir, \"func\", \"t_\" + rm_ext(img)+\".nii.gz\")\n",
    "    \n",
    "    command=\"3dTshift -TR {}s -prefix {} {}\".format(tr, tshift_path, img)\n",
    "    os.system(command)\n",
    "    print(\"       \"+command)\n",
    "\n",
    "    return(tshift_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motcor(img, out_dir):\n",
    "    motcor_path=os.path.join(out_dir, \"func\", \"m_\"+os.path.basename(img))\n",
    "    _1dfile_path=os.path.join(out_dir, \"motion\", \"1d_\"+rm_ext(os.path.basename(img))+\".1D\")\n",
    "\n",
    "    command=\"3dvolreg -base 0 -prefix {} -1Dfile {} {}\".format(motcor_path, _1dfile_path, img)\n",
    "    os.system(command)\n",
    "    print(\"       \"+command)\n",
    "    afni2nifti(motcor_path)\n",
    "\n",
    "    return(motcor_path, _1dfile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new spatial normalization\n",
    "def spatnorm(f_img, a_img, template, out_dir):\n",
    "    import os\n",
    "\n",
    "    # lin warp func to struct\n",
    "    print(\"       + Linear-warping functional to structural...\")\n",
    "    l_func_omat=os.path.join(out_dir, \"spat_norm\", \"func2str.mat\")\n",
    "    command=\"flirt -ref {} -in {} -omat {} -dof 6\".format(a_img, f_img, l_func_omat)\n",
    "    os.system(command)\n",
    "    print(\"         \"+command)\n",
    "    \n",
    "    # lin warp struct to template\n",
    "    print(\"       + Linear-warping structural to standard template...\")\n",
    "    l_anat_img =os.path.join(out_dir, \"anat\", \"l_\" + os.path.basename(a_img))\n",
    "    l_anat_omat=os.path.join(out_dir, \"spat_norm\", \"aff_str2std.mat\")\n",
    "    command=\"flirt -ref {} -in {} -omat {} -out {}\".format(template, a_img, l_anat_omat, l_anat_img)\n",
    "    os.system(command)\n",
    "    print(\"         \"+command)\n",
    "    \n",
    "    # non-lin warp struct to template\n",
    "    print(\"       + Non-linear-warping structural to standard template...\")\n",
    "    nl_anat_img  =os.path.join(out_dir, \"anat\", \"n\" + os.path.basename(l_anat_img))\n",
    "    cout_anat_img=os.path.join(out_dir, \"anat\", \"cout_\" + os.path.basename(nl_anat_img))\n",
    "    command=\"fnirt --ref={} --in={} --aff={} --iout={} --cout={} --subsamp=2,2,2,1\".format(template, a_img, l_anat_omat, nl_anat_img, cout_anat_img)\n",
    "    os.system(command)\n",
    "    print(\"         \"+command)\n",
    "    \n",
    "    # make binary mask from non-lin warped image\n",
    "    print(\"       + Creating binary mask from non-linearly warped image...\")\n",
    "    bin_nl_anat_img=os.path.join(out_dir, \"anat\", \"bin_\" + os.path.basename(nl_anat_img))\n",
    "    command=\"fslmaths {} -bin {}\".format(nl_anat_img, bin_nl_anat_img)\n",
    "    os.system(command)\n",
    "    print(\"         \"+command)\n",
    "    \n",
    "    # apply std warp to func data\n",
    "    print(\"       + Applying standardized warp to functional data...\")\n",
    "    nl_func_img=os.path.join(out_dir, \"func\", \"nl_\"+os.path.basename(f_img))\n",
    "    command=\"applywarp --ref={} --in={} --out={} --warp={} --premat={}\".format(template, f_img, nl_func_img, cout_anat_img, l_func_omat)\n",
    "    os.system(command)\n",
    "    print(\"         \"+command)\n",
    "\n",
    "    # create tempalte mask\n",
    "    print(\"       + Creating binary template mask...\")\n",
    "    mask_path=os.path.join(out_dir, \"anat\", \"mask_\" + os.path.basename(template))\n",
    "    command=\"fslmaths {} -bin {}\".format(template, mask_path)\n",
    "    os.system(command)\n",
    "    print(\"         \"+command)\n",
    "\n",
    "    return(nl_anat_img, nl_func_img, cout_anat_img, l_anat_omat) # anat, func, warp, premat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applywarp(in_img, out_img, ref_img, warp_img, premat):\n",
    "    os.system(\"fnirt --ref={} --in={} --aff={} --iout={} --cout={} --subsamp=2,2,2,1\".format(ref_img, in_img, premat, out_img, warp_img))\n",
    "    return(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(img, out_dir=None):\n",
    "    if out_dir is None:\n",
    "        out_dir = os.path.dirname(img)\n",
    "    seg_path = os.path.join(out_dir, \"seg\")\n",
    "    command=\"fast -n 3 -t 1 -o '{}' '{}'\".format(seg_path, img)\n",
    "    os.system(command)\n",
    "    print(\"       \"+command)\n",
    "\n",
    "    return(seg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_mask(mask, thr=0.5):\n",
    "    output = os.path.join(os.path.dirname(mask), \"bin_\"+os.path.basename(mask))\n",
    "    command=\"fslmaths {} -thr {} -bin {}\".format(mask, thr, output)\n",
    "    os.system(command)\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_tcourse(img, mask, save_path):\n",
    "    # compute the mean time course for ROI\n",
    "    command=\"fslmeants -i '{}' -m '{}' -o '{}'\".format(img, mask, save_path)\n",
    "    os.system(command)\n",
    "    print(\"       \"+command)\n",
    "    \n",
    "    return(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spat_smooth(img, mm, out_dir): \n",
    "    sigma=gauss_mm2sigma(mm)\n",
    "    s_img=os.path.join(out_dir, \"s_\"+os.path.basename(img))\n",
    "    \n",
    "    command=\"fslmaths {} -kernel gauss {} -fmean {}\".format(img, sigma, s_img)\n",
    "    os.system(command)\n",
    "    print(\"       \"+command)\n",
    "    \n",
    "    return(s_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_nuis(nuis1, nuis2, output):\n",
    "    if nuis1 is None:\n",
    "        nuis2_pd=pd.read_csv(nuis2, sep=\"\\s+\", header=None)\n",
    "        nuis2_pd.to_csv(output, sep=\"\\t\", header=None, index=False)\n",
    "        return(output)\n",
    "    elif nuis2 is None:\n",
    "        nuis1_pd=pd.read_csv(nuis1, sep=\"\\s+\", header=None)\n",
    "        nuis1_pd.to_csv(output, sep=\"\\t\", header=None, index=False)\n",
    "        return(output)\n",
    "\n",
    "    nuis1_pd=pd.read_csv(nuis1, sep=\"\\s+\", header=None)\n",
    "    nuis2_pd=pd.read_csv(nuis2, sep=\"\\s+\", header=None)\n",
    "\n",
    "    nuis_cbind = pd.concat([nuis1_pd, nuis2_pd], axis=1)\n",
    "    nuis_cbind.to_csv(output, sep=\"\\t\", header=None, index=False)\n",
    "\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuis_reg(img, _1d, out_dir, pref=\"nuis\", poly=\"1\"):\n",
    "\n",
    "    clean_img=os.path.join(out_dir, pref + \"_\" + rm_ext(img) + \".nii.gz\")\n",
    "    command=\"3dDeconvolve -input {} -ortvec  {} {} -polort {} -errts {}\".format(img, _1d, pref, poly, clean_img)\n",
    "    os.system(command)\n",
    "    print(\"       \"+command)\n",
    "\n",
    "    return(clean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meantsBOLD(img, outdir, nomoco):\n",
    "    dvars_txt = os.path.join(outdir, \"dvars.1D\")\n",
    "    dvars_png = os.path.join(outdir, \"dvars.png\")\n",
    "\n",
    "    out_compound = os.path.join(outdir, \"dvars_outCols.1D\")\n",
    "    out_outliers = os.path.join(outdir, \"dvars_outliers.1D\")\n",
    "\n",
    "    command=\"fsl_motion_outliers -i {} -o {} -s {} -p {} --dvars\".format(img, out_compound, dvars_txt, dvars_png)\n",
    "    if nomoco:\n",
    "        command+=\" --nomoco\"\n",
    "\n",
    "    os.system(command)\n",
    "\n",
    "    sleep(10)\n",
    "    if not os.path.exists(out_compound):\n",
    "        nlines=file_len(dvars_txt)\n",
    "        dvars_compound=np.zeros([nlines,1], dtype=int)\n",
    "        dvars_compound=pd.DataFrame(dvars_compound)\n",
    "        dvars_compound.to_csv(out_compound, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    dvars_out=pd.read_csv(out_compound, delim_whitespace=True, header=None)\n",
    "    dvars_out=dvars_out.sum(axis=1)\n",
    "    dvars_out=dvars_out.astype(\"int\")\n",
    "    dvars_out.to_csv(out_outliers, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    return(out_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd_out(img, voxel_size, outdir):\n",
    "    thresh = voxel_size/2\n",
    "    fd_txt = os.path.join(outdir, \"fd.1D\")\n",
    "    fd_png = os.path.join(outdir, \"fd.png\")\n",
    "\n",
    "    out_compound = os.path.join(outdir, \"fd_outCols.1D\")\n",
    "    out_outliers = os.path.join(outdir, \"fd_outliers.1D\")\n",
    "\n",
    "    command = \"fsl_motion_outliers -i {} -o {} -s {} -p {} --fd --thresh={}\".format(img, out_compound, fd_txt, fd_png, thresh)\n",
    "    os.system(command)\n",
    "\n",
    "    sleep(10)\n",
    "    if not os.path.exists(out_compound):\n",
    "        nlines=file_len(fd_txt)\n",
    "        fd_compound=np.zeros([nlines,1], dtype=int)\n",
    "        fd_compound=pd.DataFrame(fd_compound)\n",
    "        fd_compound.to_csv(out_compound, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    fd_out=pd.read_csv(out_compound, delim_whitespace=True, header=None)\n",
    "    fd_out=fd_out.sum(axis=1)\n",
    "    fd_out=fd_out.astype(\"int\")\n",
    "\n",
    "    fd_out.to_csv(out_outliers, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    return(out_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_outliers(dvars, fd, out_dir, method=\"UNION\"):\n",
    "    if fd is not None:\n",
    "        fd_mat = pd.read_csv(fd, delimiter=\"\\t\", header=None)\n",
    "    if dvars is not None:\n",
    "        dvars_mat = pd.read_csv(dvars, delimiter=\"\\t\", header=None)\n",
    "    \n",
    "    if method==\"UNION\":\n",
    "        outliers_mat = (fd_mat + dvars_mat).astype(\"bool\")\n",
    "        outliers_mat = outliers_mat.astype(\"int\")\n",
    "    elif method==\"INTERSECT\":\n",
    "        outliers_mat = dvars_mat[0]*fd_mat[0]\n",
    "        outliers_mat = outliers_mat.dropna()\n",
    "    elif method==\"FD\":\n",
    "        outliers_mat = fd_mat\n",
    "    elif method==\"DVARS\":\n",
    "        outliers_mat = dvars_mat\n",
    "    \n",
    "    outlier_path = os.path.join(out_dir, \"scrub_outliers.txt\")\n",
    "    outliers_mat.to_csv(outlier_path, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    return(outlier_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_time(img, out, int_ind, return_full=False):\n",
    "    if len(img.shape)!=4:\n",
    "        raise Exception(\"Nifti image must have 4 dimensions.\")\n",
    "    \n",
    "    # make blank image\n",
    "    int_dim = list(img.shape[0:3])\n",
    "    int_dim.append(int_ind[1]-int_ind[0]-1)\n",
    "    int_img = np.empty(int_dim)\n",
    "\n",
    "    for row in range(img.shape[0]):\n",
    "        for col in range(img.shape[1]):\n",
    "            for slice in range(img.shape[2]):\n",
    "                int_val = interp_pts(img[row, col, slice, int_ind[0]], img[row, col, slice, int_ind[1]], int_ind[1]-int_ind[0]-1)\n",
    "                int_img[row, col, slice, :] = int_val\n",
    "\n",
    "    if return_full:\n",
    "        img[:,:,:,out] = int_img\n",
    "        return(img)\n",
    "    else:\n",
    "        return(int_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_pts(start, end, nvals, round=True):\n",
    "    step_len =(end-start)/(nvals+1)\n",
    "    interp_set=[start+(step_len*i) for i in range(1,nvals+1)]\n",
    "\n",
    "    if round:\n",
    "        interp_set=round_dec(interp_set, [start, end])\n",
    "\n",
    "    return(interp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_dec(vals, ref):\n",
    "    n_places=max([n_dec(i) for i in ref])\n",
    "    rnd_vals=[round(i, n_places) for i in vals]\n",
    "\n",
    "    return(rnd_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_dec(val):\n",
    "    n_places=abs(decimal.Decimal(str(val)).as_tuple().exponent)\n",
    "    return(n_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrounding_timepoints(t_range, ind):   \n",
    "    sub_ind = np.empty((0,2), dtype=int, order='C')\n",
    "    \n",
    "    for ii in ind:\n",
    "        t_start = ii-1\n",
    "        t_end   = ii+1\n",
    "        \n",
    "        while t_start in ind:\n",
    "            t_start -= 1\n",
    "        while t_end in ind:\n",
    "            t_end += 1\n",
    "        \n",
    "        if t_start < min(t_range):\n",
    "            t_start = \"NaN\"\n",
    "        if t_end < min(t_range):\n",
    "            t_end = \"NaN\"\n",
    "        \n",
    "        sub_ind = np.vstack([sub_ind, (t_start, t_end)])\n",
    "        sub_ind = np.unique(sub_ind, axis=0)\n",
    "    return(sub_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrubbing(nifti, outliers, out_dir, interpolate=False):\n",
    "    img = nib.load(nifti)\n",
    "    mat = img.get_fdata()\n",
    "\n",
    "    outliers=read_csv(outliers, delimiter=\"\\t\", header=None)\n",
    "    outliers=outliers.values.tolist()\n",
    "    outliers=which([o[0] for o in outliers], 1)\n",
    "\n",
    "    print(\"       + Removing outlier timepoints...\")\n",
    "    mat[:,:,:,outliers]=float(\"NaN\")\n",
    "\n",
    "    if interpolate:\n",
    "        print(\"       + Linearly-interpolating outliers...\")\n",
    "        sub_ind = surrounding_timepoints(range(mat.shape[3]), outliers)\n",
    "\n",
    "        for oo in range(len(sub_ind)):\n",
    "            tmp = interp_time(mat, outliers[oo], sub_ind[oo,:], return_full=False)\n",
    "            mat[:,:,:,(sub_ind[oo,0]+1):sub_ind[oo,1]] = tmp\n",
    "\n",
    "    new_img = nib.Nifti1Image(mat, img.affine, header=img.header)\n",
    "    print(\"       + Saving scrubbed timeseries...\")\n",
    "    scrub_path=os.path.join(out_dir, \"scrub_\"+rm_ext(os.path.basename(nifti))+\".nii\")\n",
    "    nib.save(new_img, scrub_path)\n",
    "    \n",
    "    return(gzip_file(scrub_path))"
   ]
  },
  {
   "source": [
    "# Wrappers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def wrapper_lvl2(input_file, config_file):\n",
    "    config = interpret_config(config_file)\n",
    "    input  = interpret_input(input_file)\n",
    "    nrow   = len(input.index)\n",
    "    \n",
    "    for row in range(nrow):\n",
    "        wrapper_lvl1(input.loc[row,:], config)\n",
    "        print(\"\")\n",
    "    return(input, config)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_lvl1(input, config):\n",
    "    create_dirstruct(input[\"OUTPUT\"])\n",
    "\n",
    "    # logging the print statements\n",
    "    olog = os.path.join(input[\"OUTPUT\"], \"stdout.log\")\n",
    "    open(olog, \"w\").close()\n",
    "    sys.stdout = open(olog, \"w\")\n",
    "\n",
    "    # copy files to new locations\n",
    "    cur_func=cp_rename(os.path.join(input[\"OUTPUT\"], \"func\", \"func\"   + get_ext(input[\"FUNC\"])), input[\"FUNC\"])\n",
    "    cur_anat=cp_rename(os.path.join(input[\"OUTPUT\"], \"anat\", \"MPRage\" + get_ext(input[\"FUNC\"])), input[\"ANAT\"])\n",
    "\n",
    "    # create empty dictionary\n",
    "    pipe_steps={}\n",
    "\n",
    "    # add voxel size\n",
    "    voxel_sz=voxel_size(cur_func, excl_time=False)\n",
    "\n",
    "    # sort step keys by value\n",
    "    sorted_steps = sorted(step_order.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    for step_key in sorted_steps:\n",
    "        step_key = step_key[0]\n",
    "\n",
    "        # add baseline\n",
    "        if step_key==\"BASELINE\":\n",
    "            pipe_steps={\"BASELINE\":{\"func\":cur_func, \"anat\":cur_anat, \"voxel_size\":voxel_sz}}\n",
    "        \n",
    "        elif step_key==\"SKULLSTRIP\" and config[\"SKULLSTRIP\"]==\"1\": \n",
    "            # will always run on the baseline\n",
    "            print(\"SKULL STRIPPING\")\n",
    "            \n",
    "            step = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "            cur_anat = skullstrip(pipe_steps[\"BASELINE\"][\"anat\"], input[\"OUTPUT\"])\n",
    "            pipe_steps[\"SKULLSTRIP\"] = {\"anat\":cur_anat, \"func\":pipe_steps[\"BASELINE\"][\"anat\"]}\n",
    "        \n",
    "        elif step_key==\"SLICETIME\" and config[\"SLICETIME\"]==\"1\":\n",
    "            print(\"SLICETIME CORRECTION\")\n",
    "\n",
    "            step = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "            cur_func = slicetime(cur_func, input[\"OUTPUT\"])\n",
    "            pipe_steps[\"SLICETIME\"] = {\"anat\":pipe_steps[step][\"anat\"], \"func\":cur_func}\n",
    "        \n",
    "        elif step_key==\"MOTCOR\" and config[\"MOTCOR\"]==\"1\":\n",
    "            print(\"MOTION CORRECTION\")\n",
    "\n",
    "            step = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "            cur_func, _1dfile_path=motcor(pipe_steps[step][\"func\"], input[\"OUTPUT\"])\n",
    "            pipe_steps[\"MOTCOR\"] = {\"anat\":pipe_steps[step][\"anat\"], \"func\":cur_func, \"mot_estim\":_1dfile_path}\n",
    "        \n",
    "        elif step_key==\"NORM\" and config[\"NORM\"]==\"1\":\n",
    "            print(\"SPATIAL NORMALIZATION\")\n",
    "\n",
    "            step = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "            cur_anat, cur_func, nl_warp, nl_premat = spatnorm(pipe_steps[step][\"func\"], pipe_steps[step][\"anat\"], config[\"TEMPLATE\"], input[\"OUTPUT\"])        \n",
    "            pipe_steps[\"NORM\"]={\"anat\":cur_anat, \"func\":cur_func, \"nl_warp\":nl_warp, \"nl_premat\":nl_premat}\n",
    "        \n",
    "        elif step_key==\"NUISANCE\" and config[\"NUISANCE\"]!=\"0\":\n",
    "            print(\"NUISANCE SIGNAL REGRESSION\")\n",
    "            \n",
    "            step = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "            nuis_path=os.path.join(input[\"OUTPUT\"], \"motion\", \"nuisance_regressors.1D\")\n",
    "\n",
    "            if config[\"GSR\"]==\"1\":\n",
    "                print(\"       + Global Signal Regression...\")\n",
    "                \n",
    "                step = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "                csf_mask=segment(pipe_steps[step][\"anat\"], os.path.join(input[\"OUTPUT\"], \"anat\", \"segment\")) + \"_pve_0.nii.gz\"\n",
    "                bin_csf_mask=bin_mask(csf_mask, 0.75)\n",
    "                \n",
    "                gs_tcourse_path=os.path.join(input[\"OUTPUT\"], \"motion\", \"global_signal.1D\")\n",
    "                gs_tcourse=roi_tcourse(pipe_steps[step][\"func\"], bin_csf_mask, gs_tcourse_path)\n",
    "            else:\n",
    "                gs_tcourse=None\n",
    "                bin_csf_mask=None\n",
    "\n",
    "            if config[\"MOTREG\"]==\"1\":\n",
    "                print(\"       + Motion Parameter Regression...\")\n",
    "                mot_tcourse=pipe_steps[\"MOTCOR\"][\"mot_estim\"]\n",
    "            else:\n",
    "                mot_tcourse=None\n",
    "\n",
    "            if mot_tcourse is None and gs_tcourse is None:\n",
    "                raise ValueError(\"Must enable at least one of the following to perform nuisance regression: MOTCOR, GSR\")\n",
    "            \n",
    "            nuis_tab=combine_nuis(mot_tcourse, gs_tcourse, nuis_path)\n",
    "            cur_func=nuis_reg(pipe_steps[step][\"func\"], nuis_tab, os.path.join(input[\"OUTPUT\"],\"func\"), pref=\"nuis\",poly=config[\"NUISANCE\"])\n",
    "\n",
    "            pipe_steps[\"NUISANCE\"]={\"anat\":pipe_steps[step][\"anat\"], \"func\":cur_func, \"csf_mask\":bin_csf_mask, \"gsr\":gs_tcourse, \"mot_reg\":mot_tcourse, \"nuis_reg\":nuis_tab}    \n",
    "\n",
    "        elif step_key==\"SMOOTH\" and float(config[\"SMOOTH\"]) > 0:\n",
    "            print(\"SPATIAL SMOOTHING\")\n",
    "\n",
    "            step = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "            cur_func=spat_smooth(pipe_steps[step][\"func\"], float(config[\"SMOOTH\"]), os.path.join(input[\"OUTPUT\"], \"func\"))\n",
    "            pipe_steps[\"SMOOTH\"] = {\"anat\":pipe_steps[step][\"anat\"], \"func\":cur_func}\n",
    "        \n",
    "        elif step_key==\"SCRUB\" and config[\"SCRUB\"]!=\"NONE\":\n",
    "            print(\"SCRUBBING fMRI TIME SERIES\")\n",
    "            step = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "\n",
    "            if config[\"SCRUB\"] in [\"UNION\", \"INTERSECT\", \"FD\"]:\n",
    "                print(\"       + Frame-wise Displacement...\")\n",
    "                fd_outliers = fd_out(img=pipe_steps[\"BASELINE\"][\"func\"], voxel_size=sum(voxel_sz)/len(voxel_sz), outdir=os.path.join(input[\"OUTPUT\"], \"motion\"))\n",
    "            else:\n",
    "                fd_outliers = None\n",
    "\n",
    "            if config[\"SCRUB\"] in [\"UNION\", \"INTERSECT\", \"DVARS\"]:\n",
    "                print(\"       + DVARS...\")\n",
    "                nomoco = config[\"MOTCOR\"]==\"1\"\n",
    "                if nomoco:\n",
    "                    cur_func=pipe_steps[\"MOTCOR\"][\"func\"]\n",
    "                else:\n",
    "                    cur_func=pipe_steps[\"BASELINE\"][\"func\"]\n",
    "                dvar_outliers = meantsBOLD(cur_func, os.path.join(input[\"OUTPUT\"], \"motion\"), nomoco)\n",
    "            else:\n",
    "                dvars_outliers = None\n",
    "\n",
    "            scrub_outliers = mk_outliers(dvar_outliers, fd_outliers, os.path.join(input[\"OUTPUT\"], \"motion\"), method=config[\"SCRUB\"])\n",
    "            scrubbed_nifti = scrubbing(pipe_steps[step][\"func\"], scrub_outliers, os.path.join(input[\"OUTPUT\"], \"func\"), interpolate=True)\n",
    "\n",
    "            pipe_steps[\"SCRUB\"] = {\"anat\":pipe_steps[step][\"anat\"], \"func\":scrubbed_nifti, \"scrub_outliers\":scrub_outliers, \"fd\":fd_outliers, \"dvars\":dvar_outliers, \"method\":config[\"SCRUB\"]}\n",
    "\n",
    "    sys.stdout.close() "
   ]
  },
  {
   "source": [
    "# Quality Control"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ica(img, output, n_ic, mask=None):\n",
    "    ica_img = os.path.join(output, \"IC_\"+os.path.basename(img))\n",
    "\n",
    "    if mask is None:\n",
    "        canica=CanICA(n_components=n_ic, memory_level=0, random_state=0, n_jobs=1, n_init=10)\n",
    "    else:\n",
    "        canica=CanICA(n_components=n_ic, memory_level=0, mask=mask, random_state=0, n_jobs=1, n_init=10)\n",
    "\n",
    "    print(\"INDEPENDENT COMPONENTS ANALYSIS\")\n",
    "    canica.fit(img)\n",
    "\n",
    "    canica.components_img_.to_filename(ica_img)\n",
    "    \n",
    "    ica_fig_ls = []\n",
    "\n",
    "    for i, cur_img in enumerate(iter_img(canica.components_img_)):\n",
    "        ica_jpg = os.path.join(output, \"IC\"+str(i)+\"_\"+rm_ext(os.path.basename(img))+\".jpg\")\n",
    "        plot_stat_map(cur_img, display_mode=\"ortho\", title=\"IC %d\" % i, colorbar=True, output_file=ica_jpg)\n",
    "        ica_fig_ls.append(ica_jpg)\n",
    "\n",
    "    # add in plotting feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuis_correl(img, nuis, outdir, labels=None, mask=None):\n",
    "    if mask is not None:\n",
    "        tcourse=roi_tcourse(img, mask, os.path.join(outdir, \"brain_tcourse.1D\"))\n",
    "        tcourse=pd.read_csv(tcourse, sep=\"\\s+\", header=None)\n",
    "    else:\n",
    "        tcourse=nib.load(img)\n",
    "        if len(tcourse.shape) != 4:\n",
    "            raise Exception(\"The nifti file provided must be 4D.\")\n",
    "        tcourse=tcourse.get_fdata()\n",
    "        for ii in range(3):\n",
    "            tcourse=np.mean(tcourse, axis=0)\n",
    "\n",
    "    # import nuisance variables\n",
    "    nuis_mat=pd.read_csv(nuis, sep=\"\\s+\", header=None)\n",
    "\n",
    "    # computing the time course/motion parameter correlation\n",
    "    nuis_correls=np.corrcoef(x=tcourse, y=nuis_mat, rowvar=False)[1:,0]\n",
    "\n",
    "    return(nuis_correls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_preproc(pipe_steps, input, config):\n",
    "    step_fin = get_step(pipe_steps, step_order, which=\"prev\", ignore=None)\n",
    "\n",
    "    # ICA\n",
    "    if config[\"NORM\"]==\"1\":\n",
    "        ica_path = ica(pipe_steps[step_fin][\"func\"], os.path.join(input[\"OUTPUT\"], \"quality_control\"), int(config[\"IC\"]), pipe_steps[\"NORM\"][mask])\n",
    "    else:\n",
    "        ica_path = ica(pipe_steps[step_fin][\"func\"], os.path.join(input[\"OUTPUT\"], \"quality_control\"), int(config[\"IC\"]))"
   ]
  }
 ]
}